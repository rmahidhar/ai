{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14c4a65f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the training set of bigrams\n",
    "def get_bigrams_training_set():\n",
    "    xs, ys = [], []\n",
    "    words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "    chars = sorted(list(set(\"\".join(words))))\n",
    "    stoi = {s: i+1 for i,s in enumerate(chars)}\n",
    "    stoi[\".\"] = 0\n",
    "    itos = {i: s for s,i in stoi.items()}\n",
    "    for word in words[1:]:\n",
    "        chs = ['.'] + list(word) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):        \n",
    "            idx1 = stoi[ch1]\n",
    "            idx2 = stoi[ch2]\n",
    "            xs.append(idx1)\n",
    "            ys.append(idx2)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = [], []\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s,i in stoi.items()}\n",
    "for word in words[:1]:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):        \n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs)\n",
    "print(ys)\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W=tensor([[ 0.3789],\n",
      "        [ 0.1689],\n",
      "        [-1.5193],\n",
      "        [ 1.7741],\n",
      "        [ 0.4802],\n",
      "        [-0.5123],\n",
      "        [ 1.8850],\n",
      "        [-2.4840],\n",
      "        [ 0.5489],\n",
      "        [-0.7584],\n",
      "        [-0.5671],\n",
      "        [-0.4116],\n",
      "        [-1.5375],\n",
      "        [-1.1800],\n",
      "        [ 0.6047],\n",
      "        [-0.3427],\n",
      "        [ 1.7951],\n",
      "        [ 1.6558],\n",
      "        [-0.0709],\n",
      "        [ 1.7074],\n",
      "        [ 0.9136],\n",
      "        [ 0.2981],\n",
      "        [-1.1702],\n",
      "        [ 0.9223],\n",
      "        [ 1.3137],\n",
      "        [-0.2062],\n",
      "        [-0.9943]])\n",
      "value=tensor([[ 0.3789],\n",
      "        [-0.5123],\n",
      "        [-1.1800],\n",
      "        [-1.1800],\n",
      "        [ 0.1689]])\n"
     ]
    }
   ],
   "source": [
    "# creating a neural network\n",
    "\n",
    "# create a weight column vector 27x1 for a single neuron\n",
    "W = torch.randn((27, 1))\n",
    "print(f\"{W=}\")\n",
    "value = xenc @ W # (5, 27) @ (27, 1) => (5, 1)\n",
    "print(f\"{value=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "         -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
       "         -4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01,\n",
       "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0731e-02,\n",
       "          2.4968e+00,  2.4448e+00],\n",
       "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "         -2.1259e+00,  9.6041e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
       "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
       "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
       "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
       "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
       "          5.4108e-01, -1.1646e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create weights for 27 neurons\n",
    "# Randomly initilaize 27 neurons weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)\n",
    "xenc @ W # (5, 27) @ (27, 27) => (5, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0379)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every element in xenc @ W telling us, for every one of 27 neurons that we created \n",
    "# what is the firing rate of those neurons on every one of those five examples\n",
    "# the elment [3, 13] gives us the firing rate of 13th neuron looking at 3rd input\n",
    "\n",
    "(xenc @ W)[3, 13]\n",
    "\n",
    "# The firing rate of [3, 13] is acheived by the dot product of 3rd input and the\n",
    "# 13th column of the w matrix which is weights of the 13th neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result of xenc @ W is a log count and to convert log counts to count we use exponent function\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = (xenc @ W) # predict log-counts\n",
    "counts = logits.exp() # This equivalent to N matrix in statistical bigram model\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilites for next charcter\n",
    "# the last 2 lines here are together called a 'softmax'\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "bigram example 1: .e (indices 0, 5)\n",
      "input to the neural net: 0\n",
      "output probabilites from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the correct character: 0.012286253273487091\n",
      "log likelihood: -4.3992743492126465\n",
      "negative log likelihood: 4.3992743492126465\n",
      "----------------------\n",
      "bigram example 2: em (indices 5, 13)\n",
      "input to the neural net: 5\n",
      "output probabilites from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the correct character: 0.018050702288746834\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "----------------------\n",
      "bigram example 3: mm (indices 13, 13)\n",
      "input to the neural net: 13\n",
      "output probabilites from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "----------------------\n",
      "bigram example 4: ma (indices 13, 1)\n",
      "input to the neural net: 13\n",
      "output probabilites from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the correct character: 0.07367684692144394\n",
      "log likelihood: -2.6080667972564697\n",
      "negative log likelihood: 2.6080667972564697\n",
      "----------------------\n",
      "bigram example 5: a. (indices 1, 0)\n",
      "input to the neural net: 1\n",
      "output probabilites from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the correct character: 0.0149775305762887\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=======================\n",
      "average negative log likelihood, i.e loss= 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print(\"----------------------\")\n",
    "    print(f\"bigram example {i+1}: {itos[x]}{itos[y]} (indices {x}, {y})\")\n",
    "    print(\"input to the neural net:\", x)\n",
    "    print(\"output probabilites from the neural net:\", probs[i])\n",
    "    print(\"label (actual next character):\", y)\n",
    "    p = probs[i, y]\n",
    "    print(\"probability assigned by the net to the correct character:\", p.item())\n",
    "    logp = torch.log(p)\n",
    "    print(\"log likelihood:\", logp.item())\n",
    "    nll = -logp\n",
    "    print(\"negative log likelihood:\", nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"=======================\")\n",
    "print(\"average negative log likelihood, i.e loss=\", nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = (xenc @ W) # predict log-counts\n",
    "counts = logits.exp() # This equivalent to N matrix in statistical bigram model\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilites for next charcter\n",
    "# the last 2 lines here are together called a 'softmax'\n",
    "probs[torch.arange(5), ys] # print probabilites assigned by the nn for the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys].log().mean() # (negative log likelihood)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initilaize 27 neurons weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = (xenc @ W) # predict log-counts\n",
    "counts = logits.exp() # This equivalent to N matrix in statistical bigram model\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilites for next charcter\n",
    "# the last 2 lines here are together called a 'softmax'\n",
    "probs[torch.arange(5), ys] # print probabilites assigned by the nn for the next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean() # (negative log likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7492129802703857\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # set to zero the gradient\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything togeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s,i in stoi.items()}\n",
    "\n",
    "# Create the dataset\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples:\", num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953809738159\n",
      "3.371100902557373\n",
      "3.154043197631836\n",
      "3.020373821258545\n",
      "2.927711248397827\n",
      "2.8604023456573486\n",
      "2.8097290992736816\n",
      "2.7701022624969482\n",
      "2.7380728721618652\n",
      "2.711496591567993\n",
      "2.6890032291412354\n",
      "2.6696882247924805\n",
      "2.6529300212860107\n",
      "2.638277292251587\n",
      "2.6253879070281982\n",
      "2.6139907836914062\n",
      "2.60386323928833\n",
      "2.5948219299316406\n",
      "2.5867116451263428\n",
      "2.5794036388397217\n",
      "2.572789192199707\n",
      "2.5667762756347656\n",
      "2.5612878799438477\n",
      "2.5562586784362793\n",
      "2.551633596420288\n",
      "2.547365665435791\n",
      "2.543415069580078\n",
      "2.5397486686706543\n",
      "2.5363364219665527\n",
      "2.533154249191284\n",
      "2.5301806926727295\n",
      "2.5273966789245605\n",
      "2.5247862339019775\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810489654541\n",
      "2.513878345489502\n",
      "2.512052059173584\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071346759796143\n",
      "2.5056614875793457\n",
      "2.504261016845703\n",
      "2.5029289722442627\n",
      "2.5016608238220215\n",
      "2.5004522800445557\n",
      "2.4992988109588623\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.4961376190185547\n",
      "2.495173692703247\n",
      "2.4942493438720703\n",
      "2.493363380432129\n",
      "2.4925124645233154\n",
      "2.4916954040527344\n",
      "2.4909095764160156\n",
      "2.4901537895202637\n",
      "2.4894261360168457\n",
      "2.488725423812866\n",
      "2.4880495071411133\n",
      "2.4873974323272705\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855730533599854\n",
      "2.4850049018859863\n",
      "2.484455108642578\n",
      "2.4839231967926025\n",
      "2.483408212661743\n",
      "2.4829084873199463\n",
      "2.482424736022949\n",
      "2.48195481300354\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210304260254\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790265560150146\n",
      "2.4786536693573\n",
      "2.478290557861328\n",
      "2.4779367446899414\n",
      "2.4775924682617188\n",
      "2.4772567749023438\n",
      "2.4769303798675537\n",
      "2.476611375808716\n",
      "2.4763011932373047\n",
      "2.4759979248046875\n",
      "2.4757022857666016\n",
      "2.4754140377044678\n",
      "2.475132942199707\n",
      "2.474858045578003\n",
      "2.4745898246765137\n",
      "2.474327564239502\n",
      "2.474071741104126\n",
      "2.4738216400146484\n",
      "2.4735772609710693\n",
      "2.4733381271362305\n",
      "2.47310471534729\n",
      "2.4728763103485107\n",
      "2.4726526737213135\n",
      "2.4724342823028564\n",
      "2.4722204208374023\n",
      "2.472010850906372\n",
      "2.471806049346924\n",
      "2.4716055393218994\n",
      "2.4714088439941406\n",
      "2.4712166786193848\n",
      "2.4710280895233154\n",
      "2.470843553543091\n",
      "2.4706623554229736\n",
      "2.4704854488372803\n",
      "2.4703118801116943\n",
      "2.4701414108276367\n",
      "2.4699742794036865\n",
      "2.4698104858398438\n",
      "2.4696500301361084\n",
      "2.469492197036743\n",
      "2.4693379402160645\n",
      "2.4691860675811768\n",
      "2.4690370559692383\n",
      "2.468891143798828\n",
      "2.468747854232788\n",
      "2.46860671043396\n",
      "2.46846866607666\n",
      "2.468332529067993\n",
      "2.4681990146636963\n",
      "2.4680678844451904\n",
      "2.4679391384124756\n",
      "2.4678127765655518\n",
      "2.46768856048584\n",
      "2.4675662517547607\n",
      "2.4674463272094727\n",
      "2.4673285484313965\n",
      "2.467211961746216\n",
      "2.4670979976654053\n",
      "2.4669854640960693\n",
      "2.4668750762939453\n",
      "2.466766834259033\n",
      "2.4666597843170166\n",
      "2.466554641723633\n",
      "2.466451406478882\n",
      "2.4663493633270264\n",
      "2.4662487506866455\n",
      "2.4661502838134766\n",
      "2.4660532474517822\n",
      "2.4659576416015625\n",
      "2.4658634662628174\n",
      "2.465770721435547\n",
      "2.465679407119751\n",
      "2.4655895233154297\n",
      "2.465500593185425\n",
      "2.4654133319854736\n",
      "2.465327501296997\n",
      "2.465242385864258\n",
      "2.4651591777801514\n",
      "2.4650766849517822\n",
      "2.4649956226348877\n",
      "2.4649155139923096\n",
      "2.464836359024048\n",
      "2.4647586345672607\n",
      "2.464682102203369\n",
      "2.464606285095215\n",
      "2.464531660079956\n",
      "2.4644579887390137\n",
      "2.464385747909546\n",
      "2.4643139839172363\n",
      "2.464242935180664\n",
      "2.4641737937927246\n",
      "2.464104652404785\n",
      "2.4640369415283203\n",
      "2.4639699459075928\n",
      "2.4639036655426025\n",
      "2.4638383388519287\n",
      "2.4637739658355713\n",
      "2.463710308074951\n",
      "2.4636473655700684\n",
      "2.463585615158081\n",
      "2.4635238647460938\n",
      "2.463463544845581\n",
      "2.4634037017822266\n",
      "2.4633448123931885\n",
      "2.4632863998413086\n",
      "2.463228940963745\n",
      "2.4631717205047607\n",
      "2.4631152153015137\n",
      "2.463060140609741\n",
      "2.4630048274993896\n",
      "2.4629507064819336\n",
      "2.4628970623016357\n",
      "2.462843894958496\n",
      "2.462791681289673\n",
      "2.462739944458008\n",
      "2.462688446044922\n",
      "2.4626376628875732\n",
      "2.462587594985962\n",
      "2.462538242340088\n",
      "2.462489128112793\n",
      "2.4624407291412354\n",
      "2.462393045425415\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(200):\n",
    "\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network, one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivlanet to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # # (negative log likelihood) using probabilites assigned by the nn for the next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "prelay.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n",
      "dedainrwieta.\n"
     ]
    }
   ],
   "source": [
    "# generates names from NN weights\n",
    "def generate_names_from_weights(W):\n",
    "    idx = 0 # \".\" \n",
    "    name = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        name.append(itos[idx])\n",
    "        # check for end token\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(\"\".join(name))\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(10):\n",
    "    generate_names_from_weights(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
